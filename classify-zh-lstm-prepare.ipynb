{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "sys.version_info(major=3, minor=6, micro=2, releaselevel='final', serial=0)\n",
      "matplotlib 3.1.2\n",
      "numpy 1.18.0\n",
      "pandas 0.25.3\n",
      "sklearn 0.22\n",
      "tensorflow 2.0.0\n",
      "tensorflow_core.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 大纲\n",
    "##  1、载入数据\n",
    "##   1.1、数据处理，提取所有类别,并转化为类别词表\n",
    "##   1.2、数据分词，并存储，为训练时加载使用\n",
    "##   1.3、为构建词表使用，并存储词表  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_val length: 5000\n",
      "X_val length: 5000\n",
      "label_dict: {'体育': 1, '娱乐': 2, '家居': 3, '房产': 4, '教育': 5, '时尚': 6, '时政': 7, '游戏': 8, '科技': 9, '财经': 10}\n"
     ]
    }
   ],
   "source": [
    "dir_train='data\\\\cnews.train.txt'\n",
    "dir_val='data\\\\cnews.val.txt'\n",
    "dir_test='data\\\\cnews.test.txt'\n",
    "\n",
    "# prepare data output files\n",
    "seg_train_file = 'data\\\\cnews.train.seg.txt'\n",
    "seg_val_file = 'data\\\\cnews.val.seg.txt'\n",
    "seg_test_file = 'data\\\\cnews.test.seg.txt'\n",
    "\n",
    "vocab_file = 'data\\\\cnews.vocab.txt'\n",
    "label_vocab_file = 'data\\\\cnews.label.vocab.txt'\n",
    "\n",
    "def load_data(path,num_lines):\n",
    "    lines = open(path,encoding='UTF-8').read().strip().split('\\n')\n",
    "    example_pairs = [ [w for w in line.split('\\t')] for line in lines[:num_lines]]\n",
    "    return zip(*example_pairs)\n",
    "\n",
    "def label_dict(y_val):\n",
    "    index=1\n",
    "    dd={}\n",
    "    for label in y_val:\n",
    "        if label not in dd.keys():\n",
    "            dd[label]=index\n",
    "            index +=1\n",
    "    return dd\n",
    "\n",
    "y_val,X_val= load_data(dir_val,None)\n",
    "print('y_val length:',len(y_val))\n",
    "print('X_val length:',len(X_val))\n",
    "\n",
    "label_dict=label_dict(y_val)\n",
    "print('label_dict:',label_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "体育\t1\n",
      "娱乐\t2\n",
      "家居\t3\n",
      "房产\t4\n",
      "教育\t5\n",
      "时尚\t6\n",
      "时政\t7\n",
      "游戏\t8\n",
      "科技\t9\n",
      "财经\t10\n"
     ]
    }
   ],
   "source": [
    "## 保存label文件内容\n",
    "with open(label_vocab_file, 'w', encoding='utf-8') as f:\n",
    "    f.seek(0) \n",
    "    f.truncate()   #清空文件\n",
    "    for label,value in label_dict.items():\n",
    "        line = '%s\\t%d\\n' % (label,value)\n",
    "        print('%s\\t%d' % (label,value))\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\jandy\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.811 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成train数据分词重写！\n",
      "完成val数据分词重写！\n",
      "完成test数据分词重写！\n"
     ]
    }
   ],
   "source": [
    "## 做分词处理\n",
    "## jieba分词\n",
    "import jieba\n",
    "def generate_seg_file(input_file, output_seg_file):\n",
    "    \"\"\"Segment the sentences in each line in input_file\"\"\"\n",
    "    with open(input_file, 'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    with open(output_seg_file, 'w',encoding='utf-8') as f:\n",
    "        f.seek(0) \n",
    "        f.truncate()   #清空文件\n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            word_iter = jieba.cut(content)\n",
    "            word_content = ''\n",
    "            for word in word_iter:\n",
    "                word = word.strip(' ')\n",
    "                if word != '':\n",
    "                    word_content += word + ' '\n",
    "            out_line = '%s\\t%s\\n' % (label, word_content.strip(' '))\n",
    "            f.write(out_line)\n",
    "\n",
    "## 时间比较长，只需执行一次\n",
    "generate_seg_file(dir_train, seg_train_file)\n",
    "print('完成train数据分词重写！')\n",
    "generate_seg_file(dir_val, seg_val_file)\n",
    "print('完成val数据分词重写！')\n",
    "generate_seg_file(dir_test, seg_test_file)\n",
    "print('完成test数据分词重写！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict length: 359260\n"
     ]
    }
   ],
   "source": [
    "## 做词表,就是词出现的次数频率\n",
    "def generate_vocab_file(input_seg_file, output_vocab_file):\n",
    "    with open(input_seg_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    word_dict = {}\n",
    "    for line in lines:\n",
    "        label, content = line.strip('\\r\\n').split('\\t')\n",
    "        for word in content.split():\n",
    "            word_dict.setdefault(word, 0)\n",
    "            word_dict[word] += 1\n",
    "    # [(word, frequency), ..., ()]\n",
    "    ##  按词频排序\n",
    "    sorted_word_dict = sorted(\n",
    "        word_dict.items(), key = lambda d:d[1], reverse=True)\n",
    "    with open(output_vocab_file, 'w',encoding='utf-8') as f:\n",
    "        f.seek(0) \n",
    "        f.truncate()   #清空文件\n",
    "        f.write('<UNK>\\t10000000\\n')\n",
    "        for item in sorted_word_dict:\n",
    "            f.write('%s\\t%d\\n' % (item[0], item[1]))\n",
    "    print('dict length:',len(sorted_word_dict))\n",
    "\n",
    "generate_vocab_file(seg_train_file, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 另一种词表方案\n",
    "vocab_file_simple = 'data\\\\cnews.vocab_simple.txt'\n",
    "## 构建分词器\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "with open(seg_train_file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    example_pairs=[[example for example in line.strip('\\r\\n').split('\\t')] for line in lines]\n",
    "    y,X_train_seg=zip(*example_pairs)\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('马晓旭 意外 受伤 让 国奥 警惕 无奈 大雨 格外 青睐 殷家 军 记者 傅亚雨 沈阳 报道 来到 沈阳 ， 国奥队 依然 没有 摆脱 雨水 的 困扰 。 7 月 31 日 下午 6 点 ， 国奥队 的 日常 训练 再度 受到 大雨 的 干扰 ， 无奈 之下 队员 们 只 慢跑 了 25 分钟 就 草草收场 。 31 日 上午 10 点 ， 国奥队 在 奥体中心 外场 训练 的 时候 ， 天 就是 阴沉沉 的 ， 气象预报 显示 当天 下午 沈阳 就 有 大雨 ， 但 幸好 队伍 上午 的 训练 并 没有 受到 任何 干扰 。 下午 6 点 ， 当 球队 抵达 训练场 时 ， 大雨 已经 下 了 几个 小时 ， 而且 丝毫 没有 停下来 的 意思 。 抱 着 试一试 的 态度 ， 球队 开始 了 当天 下午 的 例行 训练 ， 25 分钟 过去 了 ， 天气 没有 任何 转好 的 迹象 ， 为了 保护 球员 们 ， 国奥队 决定 中止 当天 的 训练 ， 全队 立即 返回 酒店 。 在 雨 中 训练 对 足球队 来说 并 不是 什么 稀罕 事 ， 但 在 奥运会 即将 开始 之前 ， 全队 变得 “ 娇贵 ” 了 。 在 沈阳 最后 一周 的 训练 ， 国奥队 首先 要 保证 现有 的 球员 不再 出现意外 的 伤病 情况 以免 影响 正式 比赛 ， 因此 这一 阶段 控制 训练 受伤 、 控制 感冒 等 疾病 的 出现 被 队伍 放在 了 相当 重要 的 位置 。 而 抵达 沈阳 之后 ， 中 后卫 冯萧霆 就 一直 没有 训练 ， 冯萧霆 是 7 月 27 日 在 长春 患上 了 感冒 ， 因此 也 没有 参加 29 日 跟 塞尔维亚 的 热身赛 。 队伍 介绍 说 ， 冯萧霆 并 没有 出现 发烧 症状 ， 但 为了 安全 起 见 ， 这 两天 还是 让 他 静养 休息 ， 等 感冒 彻底 好 了 之后 再 恢复 训练 。 由于 有 了 冯萧霆 这个 例子 ， 因此 国奥队 对雨中 训练 就 显得 特别 谨慎 ， 主要 是 担心 球员 们 受凉 而 引发 感冒 ， 造成 非战斗 减员 。 而 女足 队员 马晓旭 在 热身赛 中 受伤 导致 无缘 奥运 的 前科 ， 也 让 在 沈阳 的 国奥队 现在 格外 警惕 ， “ 训练 中 不断 嘱咐 队员 们 要 注意 动作 ， 我们 可 不能 再出 这样 的 事情 了 。 ” 一位 工作人员 表示 。 从 长春 到 沈阳 ， 雨水 一路 伴随 着 国奥队 ， “ 也 邪 了 ， 我们 走 到 哪儿 雨 就 下 到 哪儿 ， 在 长春 几次 训练 都 被 大雨 给 搅和 了 ， 没想到 来 沈阳 又 碰到 这种 事情 。 ” 一位 国奥 球员 也 对 雨水 的 “ 青睐 ” 有些 不解 。',)\n",
      "vocab_size: 64965\n",
      "sub_words length: 64708\n"
     ]
    }
   ],
   "source": [
    "            \n",
    "print(X_train_seg[:1])\n",
    "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(X_train_seg, target_vocab_size= 2**16)\n",
    "print('vocab_size:',tokenizer.vocab_size)\n",
    "sub_words=tokenizer.subwords\n",
    "print('sub_words length:',len(sub_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ， \n",
      "的_\n",
      " 。 \n",
      "在_\n",
      " 、 \n",
      "是_\n",
      "了_\n",
      "和_\n",
      "也_\n",
      " “ \n",
      "dict length: 64708\n"
     ]
    }
   ],
   "source": [
    "for item in sub_words[:10]:\n",
    "    print(item)\n",
    "with open(vocab_file_simple, 'w',encoding='utf-8') as f:\n",
    "    f.seek(0) \n",
    "    f.truncate()   #清空文件\n",
    "    f.write('<UNK>\\t10000000\\n')\n",
    "    for index,item in enumerate(sub_words):\n",
    "        f.write('%s\\t%d\\n' % (item,index))\n",
    "    print('dict length:',len(sub_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
